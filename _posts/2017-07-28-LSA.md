---
layout: post
title: How does LSA classify documents?
subtitle: Simplest NLP technique 
bigimg: /img/ducks.jpg
---


Image you have a bunch of documents that you'd like to classify  based on their topics. The intuitive idea is to pick up keywords of each document, then cluster them into different categories. While you can't pick them up manually, but you could represent each documents using a matrix. The matrix is called term-document matrix or occurrence matrix.

#### Term-Document Matrix

 Literally, this matrix represents the frequency of each word in each document. The rows represent the terms, while the colomns represent the documents. More specifically, after you get those documents, e.g. 100 documents, you could build the corresponding vocabulary, e.g. 5000 words. Each document could be represented as a vector with dimension of 5000, with every element representing the frequency of corresponding word. Vice versa, every word could also be represented as a vector with dimension of 100, with element representing the frequency of that word in corresponding document.

Now we have a matrix, looks like:


<!--$$\begin{bmatrix}
1&0&3&5&9&\ldots\\
2&1&5&3&4&\ldots\\
3&6&4&2&1\\
\vdots&\vdots&\vdots&\vdots&\vdots&\ldots\end{bmatrix}$$-->

![](https://ws3.sinaimg.cn/large/006tKfTcgy1fi0nnx92exj30ue0ai0tv.jpg)

The LSA assumes that words that are closed in meaning should appear with a similar times among documents. In this matrix, if there is a word 'he', which is the row of the matrix, to be (1, 0, 3, 5, 9,...), similary to word 'I'. We're pretty confident that the word 'he' is closed to word 'I'. 

#### Cosine Similarity

To measure the similarity between two words or documents, the common way is to calculate the normalized inner product between two embedding vectors, which could reflect the angle between two vectors.

<!--$$similarity = cos(\theta) = \frac{\mathbf{w_1  \bullet  w_2}}{\|\mathbf{w_1}\|\bullet\|\mathbf{w_2}\|}$$
-->

<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mstyle displaystyle="true">
    <mrow class="MJX-TeXAtom-ORD">
      <mi>s</mi>
      <mi>i</mi>
      <mi>m</mi>
      <mi>i</mi>
      <mi>l</mi>
      <mi>a</mi>
      <mi>r</mi>
      <mi>i</mi>
      <mi>t</mi>
      <mi>y</mi>
      <mo>=</mo>
      <mi>c</mi>
      <mi>o</mi>
      <mi>s</mi>
      <mo stretchy="false">(</mo>
      <mi>&#x03B8;<!-- θ --></mi>
      <mo stretchy="false">)</mo>
      <mo>=</mo>
      <mfrac>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi mathvariant="bold">w</mi>
            <mn mathvariant="bold">1</mn>
          </msub>
          <mo>&#x2219;<!-- ∙ --></mo>
          <msub>
            <mi mathvariant="bold">w</mi>
            <mn mathvariant="bold">2</mn>
          </msub>
        </mrow>
        <mrow>
          <mo>&#x2225;<!-- ∥ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi mathvariant="bold">w</mi>
              <mn mathvariant="bold">1</mn>
            </msub>
          </mrow>
          <mo>&#x2225;<!-- ∥ --></mo>
          <mo>&#x2219;<!-- ∙ --></mo>
          <mo>&#x2225;<!-- ∥ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi mathvariant="bold">w</mi>
              <mn mathvariant="bold">2</mn>
            </msub>
          </mrow>
          <mo>&#x2225;<!-- ∥ --></mo>
        </mrow>
      </mfrac>
    </mrow>
  </mstyle>
</math>


<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 -1438.7855323575363 10104.158333333333 2177.0140647150724"><g stroke="#000000" fill="#000000" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><path stroke-width="10" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"/><path stroke-width="10" d="M-123 694Q-123 702 -118 708T-103 714Q-93 714 -88 706T-80 687T-67 660T-40 633Q-29 626 -29 615Q-29 606 -36 600T-53 590T-83 571T-121 531Q-135 516 -143 516T-157 522T-163 536T-152 559T-129 584T-116 595H-287L-458 596Q-459 597 -461 599T-466 602T-469 607T-471 615Q-471 622 -458 635H-99Q-123 673 -123 694Z" transform="translate(759,244)"/><path stroke-width="10" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" transform="translate(1042,0)"/><g transform="translate(2223,0)"><rect x="0" y="220" width="1220" height="60" stroke-width="10" stroke-linejoin="round" stroke-linecap="round"/><g transform="translate(60,676)"><path stroke-width="10" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/><g transform="translate(528,0)"><path stroke-width="10" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/><path stroke-width="10" d="M-123 694Q-123 702 -118 708T-103 714Q-93 714 -88 706T-80 687T-67 660T-40 633Q-29 626 -29 615Q-29 606 -36 600T-53 590T-83 571T-121 531Q-135 516 -143 516T-157 522T-163 536T-152 559T-129 584T-116 595H-287L-458 596Q-459 597 -461 599T-466 602T-469 607T-471 615Q-471 622 -458 635H-99Q-123 673 -123 694Z" transform="translate(567,7)"/></g></g><g transform="translate(163,-686)"><path stroke-width="10" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/><path stroke-width="10" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" transform="translate(528,0)"/></g></g><path stroke-width="10" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" transform="translate(3841,0)"/><path stroke-width="10" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" transform="translate(4902,0)"/><g transform="translate(6072,0)"><rect x="0" y="220" width="1156" height="60" stroke-width="10" stroke-linejoin="round" stroke-linecap="round"/><g transform="translate(60,676)"><path stroke-width="10" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/><g transform="translate(528,0)"><path stroke-width="10" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/><path stroke-width="10" d="M-123 694Q-123 702 -118 708T-103 714Q-93 714 -88 706T-80 687T-67 660T-40 633Q-29 626 -29 615Q-29 606 -36 600T-53 590T-83 571T-121 531Q-135 516 -143 516T-157 522T-163 536T-152 559T-129 584T-116 595H-287L-458 596Q-459 597 -461 599T-466 602T-469 607T-471 615Q-471 622 -458 635H-99Q-123 673 -123 694Z" transform="translate(503,7)"/></g></g><g transform="translate(131,-686)"><path stroke-width="10" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/><path stroke-width="10" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" transform="translate(528,0)"/></g></g><path stroke-width="10" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" transform="translate(7626,0)"/><path stroke-width="10" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" transform="translate(8687,0)"/><g transform="translate(9570,0)"><path stroke-width="10" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/><path stroke-width="10" d="M-123 694Q-123 702 -118 708T-103 714Q-93 714 -88 706T-80 687T-67 660T-40 633Q-29 626 -29 615Q-29 606 -36 600T-53 590T-83 571T-121 531Q-135 516 -143 516T-157 522T-163 536T-152 559T-129 584T-116 595H-287L-458 596Q-459 597 -461 599T-466 602T-469 607T-471 615Q-471 622 -458 635H-99Q-123 673 -123 694Z" transform="translate(497,5)"/></g></g></svg>

#### Lower matrix rank

Now how do you classify the documents or terms based on this matrix? For similar documents or terms, we are aiming to combine them. We are expecting to reduce the dimension of this matrix, that is map it into a lower rank matrix. In linear algebra, the singular value decomposition (SVD) could handle this. 

##### Theorem of SVD


For an $m \times n$ real matrix $\mathbf {M} $, whose element comes from field $ \mathbf {R}$. It can be decomposed as form:
$$\mathbf {M=U\Sigma V^{T}} $$
where:

*	$ \mathbf {U} $ is an $m \times m$ real	[unitary matrix](https://en.wikipedia.org/wiki/Unitary_matrix), more specifically, orthogonal matrix.
*  $\mathbf {V} $ is an $n\times n$ orthogonal matrix.  
*  $\mathbf {\Sigma} $ is an $m \times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal, known as singular values of $\mathbf {M}$. 


Note, the decomposition is not unique.
The columns of $\mathbf {U} $ and the columns of $\mathbf {V}$  are called the left-singular vectors and right-singular vectors of $\mathbf {M} $, respectively. The interesting thing is:

$$\mathbf{M M^{T}} = \mathbf{U \Sigma^{2} U^{T}}$$

$$\mathbf{M^{T} M} = \mathbf{V \Sigma^{2} V^{T}}$$

where $\mathbf{U}$, $\mathbf{V}$ must contain the eigenvectors of $\mathbf{M M^{T}}$ and $\mathbf{M^{T} M}$, respectively, since $\Sigma^{2}$ is diagonal matrix.

The singular value decomposition looks like:

$$\begin{bmatrix} x_{1,1}& \ldots & x_{1,n}\\
\vdots &\vdots  & \vdots \\ x_{m,1}&\ldots&x_{m,n}
\end{bmatrix} = \begin{bmatrix}\mathbf{u_{1}} & \ldots & \mathbf{u_{l}}\end{bmatrix}
\bullet 
\begin{bmatrix} \sigma_1&\ldots&0\\
\vdots& \ddots&\vdots \\
0&\ldots&\sigma_l\end{bmatrix}
\bullet
\begin{bmatrix}\mathbf{v_1}\\\vdots\\\mathbf{v_l}\end{bmatrix}
$$
In this representation, $\mathbf{u_i}$ and $\mathbf{v_i}$ are left and right singular vector. Or we could represent it in a more intuitive form:


$$\begin{bmatrix} x_{1,1}& \ldots & x_{1,n}\\
\vdots &\vdots  & \vdots \\ x_{m,1}&\ldots&x_{m,n}
\end{bmatrix} =
\begin{bmatrix}\hat{\mathbf{t_1^T}}\\\vdots\\\mathbf{\hat{t^T_m}}\end{bmatrix}
\bullet
\mathbf{\Sigma}_{m,n}
\bullet
\begin{bmatrix}\mathbf{d_1^T}&\ldots&\mathbf{d^T_n}\end{bmatrix}
$$
 


Now, our task is to approximate the original matrix $\mathbf{M}$ with lower rank matrix $\mathbf{\tilde{M}}$, which has a specific rank k. [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) suggests that picking up the largest k singular values, corresponding singular vectors to construct a truncated matrix $\tilde{\mathbf{M}}$ has a minimum error. That the row "term" vector $\mathbf{\hat{t}}_{i}^{T}$ then has {\displaystyle k} k entries mapping it to a lower-dimensional space dimensions.



#### Reference
* LSA:	[https://en.wikipedia.org/wiki/Latent_semantic_analysis]()