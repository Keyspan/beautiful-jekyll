---
layout: post
title: How does LSA classify documents?
subtitle: Simplest NLP technique 
bigimg: /img/ducks.jpg
---


Image you have a bunch of documents that you'd like to classify  based on their topics. The intuitive idea is to pick up keywords of each document, then cluster them into different categories. While you can't pick them up manually, but you could represent each documents using a matrix. The matrix is called term-document matrix or occurrence matrix.

#### Term-Document Matrix

 Literally, this matrix represents the frequency of each word in each document. The rows represent the terms, while the colomns represent the documents. More specifically, after you get those documents, e.g. 100 documents, you could build the corresponding vocabulary, e.g. 5000 words. Each document could be represented as a vector with dimension of 5000, with every element representing the frequency of corresponding word. Vice versa, every word could also be represented as a vector with dimension of 100, with element representing the frequency of that word in corresponding document.

Now we have a matrix, looks like:


<!--$$\begin{bmatrix}
1&0&3&5&9&\ldots\\
2&1&5&3&4&\ldots\\
3&6&4&2&1\\
\vdots&\vdots&\vdots&\vdots&\vdots&\ldots\end{bmatrix}$$-->

![](https://ws3.sinaimg.cn/large/006tKfTcgy1fi0nnx92exj30ue0ai0tv.jpg)

The LSA assumes that words that are closed in meaning should appear with a similar times among documents. In this matrix, if there is a word 'he', which is the row of the matrix, to be (1, 0, 3, 5, 9,...), similary to word 'I'. We're pretty confident that the word 'he' is closed to word 'I'. 

#### Cosine Similarity

To measure the similarity between two words or documents, the common way is to calculate the normalized inner product between two embedding vectors, which could reflect the angle between two vectors.

<!--$$similarity = cos(\theta) = \frac{\mathbf{w_1  \bullet  w_2}}{\|\mathbf{w_1}\|\bullet\|\mathbf{w_2}\|}$$
-->

<div align="center">
<img src="https://latex.codecogs.com/svg.latex?similarity&space;=&space;cos(\theta)&space;=&space;\frac{\mathbf{w_1&space;\bullet&space;w_2}}{\|\mathbf{w_1}\|\bullet\|\mathbf{w_2}\|}" title="similarity = cos(\theta) = \frac{\mathbf{w_1 \bullet w_2}}{\|\mathbf{w_1}\|\bullet\|\mathbf{w_2}\|}" />
</div>



#### Lower matrix rank

Now how do you classify the documents or terms based on this matrix? For similar documents or terms, we are aiming to combine them. We are expecting to reduce the dimension of this matrix, that is map it into a lower rank matrix. In linear algebra, the singular value decomposition (SVD) could handle this. 

##### - Theorem of SVD


For an <img src="https://latex.codecogs.com/svg.latex?$m\times&space;n$" title="$m\times n$" /> 
real matrix <img src="https://latex.codecogs.com/svg.latex?$\mathbf&space;{M}&space;$" title="$\mathbf {M} $" />,
 whose element comes from field <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{R}" title="\mathbf {R}" />
 . It can be decomposed as form:
<div align = "center"> 
<img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{M=U\Sigma&space;V^{T}}" title="\mathbf {M=U\Sigma V^{T}}" />
</div>

where:

*	<img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{U}" title="\mathbf {U}" />
is an <img src="https://latex.codecogs.com/svg.latex?m\times&space;m" title="m\times m" />
 real	[unitary matrix](https://en.wikipedia.org/wiki/Unitary_matrix), more specifically, orthogonal matrix.
*  <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{V}" title="\mathbf {V}" />
 is an <img src="https://latex.codecogs.com/svg.latex?n\times&space;n" title="n\times n" />
  orthogonal matrix.  
*  <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{\Sigma}" title="\mathbf {V}" />
  is an <img src="https://latex.codecogs.com/svg.latex?m\times&space;n" title="m\times n" /> 
  rectangular diagonal matrix with non-negative real numbers on the diagonal, known as singular values of <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{M}&space;$" title="$\mathbf {M} $" />. 


Note, the decomposition is not unique.
The columns of <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{U}" title="\mathbf {U}" /> and the columns of <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{V}" title="\mathbf {V}" />  are called the left-singular vectors and right-singular vectors of matrix <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{M}" title="\mathbf {M}" />, respectively. The interesting thing is:

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\mathbf{M&space;M^{T}}&space;=&space;\mathbf{U&space;\Sigma^{2}&space;U^{T}}" title="\mathbf{M M^{T}} = \mathbf{U \Sigma^{2} U^{T}}" />
</div>

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\mathbf{M^{T}&space;M}&space;=&space;\mathbf{V&space;\Sigma^{2}&space;V^{T}}" title="\mathbf{M^{T} M} = \mathbf{V \Sigma^{2} V^{T}}" />
</div>

where <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{U}" title="\mathbf {U}" />
, <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{V}" title="\mathbf {V}" />
must contain the eigenvectors of <img src="https://latex.codecogs.com/svg.latex?\mathbf{M&space;M^{T}}" title="\mathbf{M M^{T}}" /> 
and <img src="https://latex.codecogs.com/svg.latex?\mathbf{M^{T}&space;M}" title="\mathbf{M^{T} M}" />
, respectively, since <img src="https://latex.codecogs.com/svg.latex?\Sigma^{2}" title="\Sigma^{2}" />
 is diagonal matrix.

The singular value decomposition looks like:

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\begin{bmatrix}&space;x_{1,1}&&space;\ldots&space;&&space;x_{1,n}\\&space;\vdots&space;&\vdots&space;&&space;\vdots&space;\\&space;x_{m,1}&\ldots&x_{m,n}&space;\end{bmatrix}&space;=&space;\begin{bmatrix}\mathbf{u_{1}}&space;&&space;\ldots&space;&&space;\mathbf{u_{l}}\end{bmatrix}&space;\bullet&space;\begin{bmatrix}&space;\sigma_1&\ldots&0\\&space;\vdots&&space;\ddots&\vdots&space;\\&space;0&\ldots&\sigma_l\end{bmatrix}&space;\bullet&space;\begin{bmatrix}\mathbf{v_1}\\\vdots\\\mathbf{v_l}\end{bmatrix}" title="\begin{bmatrix} x_{1,1}& \ldots & x_{1,n}\\ \vdots &\vdots & \vdots \\ x_{m,1}&\ldots&x_{m,n} \end{bmatrix} = \begin{bmatrix}\mathbf{u_{1}} & \ldots & \mathbf{u_{l}}\end{bmatrix} \bullet \begin{bmatrix} \sigma_1&\ldots&0\\ \vdots& \ddots&\vdots \\ 0&\ldots&\sigma_l\end{bmatrix} \bullet \begin{bmatrix}\mathbf{v_1}\\\vdots\\\mathbf{v_l}\end{bmatrix}" /> 
</div>

In this representation, <img src="https://latex.codecogs.com/svg.latex?\mathbf{u_i}" title="\mathbf{u_i}" /> and <img src="https://latex.codecogs.com/svg.latex?\mathbf{v_i}" title="\mathbf{v_i}" /> are left and right singular vector. Or we could represent it in a more intuitive form:

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\begin{bmatrix}&space;x_{1,1}&&space;\ldots&space;&&space;x_{1,n}\\&space;\vdots&space;&\vdots&space;&&space;\vdots&space;\\&space;x_{m,1}&\ldots&x_{m,n}&space;\end{bmatrix}&space;=&space;\begin{bmatrix}\hat{\mathbf{t_1^T}}\\\vdots\\\mathbf{\hat{t^T_m}}\end{bmatrix}&space;\bullet&space;\mathbf{\Sigma}_{m,n}&space;\bullet&space;\begin{bmatrix}\mathbf{d_1^T}&\ldots&\mathbf{d^T_n}\end{bmatrix}" title="\begin{bmatrix} x_{1,1}& \ldots & x_{1,n}\\ \vdots &\vdots & \vdots \\ x_{m,1}&\ldots&x_{m,n} \end{bmatrix} = \begin{bmatrix}\hat{\mathbf{t_1^T}}\\\vdots\\\mathbf{\hat{t^T_m}}\end{bmatrix} \bullet \mathbf{\Sigma}_{m,n} \bullet \begin{bmatrix}\mathbf{d_1^T}&\ldots&\mathbf{d^T_n}\end{bmatrix}" />
</div>
 


Now, our task is to approximate the original matrix <img src="https://latex.codecogs.com/svg.latex?\mathbf{M}" title="mathbf{M}" /> with lower rank matrix <img src="https://latex.codecogs.com/svg.latex?\mathbf{\tilde{M}}" title="\mathbf{\tilde{M}}" /> , which has a specific rank <img src="https://latex.codecogs.com/svg.latex?k" title="k" /> . [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) suggests that picking up the largest <img src="https://latex.codecogs.com/svg.latex?k" title="k" /> singular values, corresponding singular vectors to construct a truncated matrix <img src="https://latex.codecogs.com/svg.latex?\mathbf{\tilde{M}}" title="\mathbf{\tilde{M}}" /> has a minimum error. That the row "term" vector <img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{t}}^{T}_{i}" title="\mathbf{\hat{t}}_{i}^{T}" /> then has <img src="https://latex.codecogs.com/svg.latex?k" title="k" /> entries mapping it to a lower-dimensional space dimensions.



#### Reference
* LSA:	[https://en.wikipedia.org/wiki/Latent_semantic_analysis]()