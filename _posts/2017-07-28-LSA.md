---
layout: post
title: How does LSA classify documents?
subtitle: Simplest NLP technique 
bigimg: /img/ducks.jpg
---
#### Introduction

Image you have a bunch of documents that you'd like to classify  based on their topics. The intuitive idea is to pick up keywords of each document, then cluster them into different categories. While you can't pick them up manually, but you could represent each documents using a matrix. The matrix is called term-document matrix or occurrence matrix. Literally, this matrix represents the frequency of each word in each document. The rows represent the terms, while the colomns represent the documents, vice vesa. More specifically, after you get those documents, e.g. 100 documents, you could build the corresponding vocabulary, e.g. 5000 words. Each document could be represented as a vector with dimension of 5000, with every element representing the frequency of corresponding word. Vice versa, every word could also be represented as a vector with dimension of 100, with element representing the frequency of that word in corresponding document.

Now we have a matrix, looks like:


<!--$$\begin{bmatrix}
1&0&3&5&9&\ldots\\
2&1&5&3&4&\ldots\\
3&6&4&2&1\\
\vdots&\vdots&\vdots&\vdots&\vdots&\ldots\end{bmatrix}$$-->


![](https://ws2.sinaimg.cn/large/006tKfTcgy1fhzh35v8w3j316k0isq5g.jpg)

The LSA assumes that words that are closed in meaning should appear with a similar times among documents. In this matrix, if there is a word 'he', which is the row of the matrix, to be (1,0,3,5,9,...), similary to word 'I'. We're pretty confident that the word 'he' is closed to word 'I'. 

#### Cosine Similarity

To measure the similarity between two words or documents, the common way is to calculate the normalized inner product between two embedding vectors, which could reflect the angle between two vectors.

$$similarity = cos(\theta) = \frac{\mathbf{w_1  \bullet  w_2}}{\|\mathbf{w_1}\|\bullet\|\mathbf{w_2}\|}$$

#### Lower matrix rank

Now how do you classify the documents or terms based on this matrix? For similar documents or terms, we are aiming to combine them. We are expecting to reduce the dimension of this matrix, that is map it into a lower rank matrix. In linear algebra, the singular value decomposition (SVD) could handle this. 

##### Theorem of SVD


For an $m \times n$ real matrix $\mathbf {M} $, whose element comes from field $ \mathbf {R}$. It can be decomposed as form:
$$\mathbf {M=U\Sigma V^{T}} $$
where:

*	$ \mathbf {U} $ is an $m \times m$ real	[unitary matrix](https://en.wikipedia.org/wiki/Unitary_matrix), more specifically, orthogonal matrix.
*  $\mathbf {V} $ is an $n\times n$ orthogonal matrix.  
*  $\mathbf {\Sigma} $ is an $m \times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal, known as singular values of $\mathbf {M}$. 

Note, the decomposition is not unique.
The columns of $\mathbf {U} $ and the columns of $\mathbf {V}$  are called the left-singular vectors and right-singular vectors of $\mathbf {M} $, respectively. The interesting thing is:

$$\mathbf{M M^{T}} = \mathbf{U \Sigma^{2} U^{T}}$$

$$\mathbf{M^{T} M} = \mathbf{V \Sigma^{2} V^{T}}$$

where $\mathbf{U}$, $\mathbf{V}$ must contain the eigenvectors of $\mathbf{M M^{T}}$ and $\mathbf{M^{T} M}$, respectively, since $\Sigma^{2}$ is diagonal matrix.

The singular value decomposition looks like:

$$\begin{bmatrix} x_{1,1}& \ldots & x_{1,n}\\
\vdots &\vdots  & \vdots \\ x_{m,1}&\ldots&x_{m,n}
\end{bmatrix} = \begin{bmatrix}\mathbf{u_{1}} & \ldots & \mathbf{u_{l}}\end{bmatrix}
\bullet 
\begin{bmatrix} \sigma_1&\ldots&0\\
\vdots& \ddots&\vdots \\
0&\ldots&\sigma_l\end{bmatrix}
\bullet
\begin{bmatrix}\mathbf{v_1}\\\vdots\\\mathbf{v_l}\end{bmatrix}
$$
In this representation, $\mathbf{u_i}$ and $\mathbf{v_i}$ are left and right singular vector. Or we could represent it in a more intuitive form:

```
$$\begin{bmatrix} x_{1,1}& \ldots & x_{1,n}\\
\vdots &\vdots  & \vdots \\ x_{m,1}&\ldots&x_{m,n}
\end{bmatrix} =
\begin{bmatrix}\hat{\mathbf{t_1^T}}\\\vdots\\\mathbf{\hat{t^T_m}}\end{bmatrix}
\bullet
\mathbf{\Sigma}_{m,n}
\bullet
\begin{bmatrix}\mathbf{d_1^T}&\ldots&\mathbf{d^T_n}\end{bmatrix}
$$
```

Now, our task is to approximate the original matrix $\mathbf{M}$ with lower rank matrix $\mathbf{\tilde{M}}$, which has a specific rank k. [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) suggests that picking up the largest k singular values, corresponding singular vectors to construct a truncated matrix $\tilde{\mathbf{M}}$ has a minimum error. That the row "term" vector $\mathbf{\hat{t}}_{i}^{T}$ then has {\displaystyle k} k entries mapping it to a lower-dimensional space dimensions.



#### Reference
* LSA:	[https://en.wikipedia.org/wiki/Latent_semantic_analysis]()