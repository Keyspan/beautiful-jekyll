---
layout: post
title: How does LSA classify documents?
subtitle: Simplest NLP technique 
bigimg: /img/ducks.jpg
---


Image you have a bunch of documents that you'd like to classify  based on their topics. The intuitive idea is to pick up keywords of each document, then cluster them into different categories. While you can't pick them up manually, but you could represent each documents using a matrix. The matrix is called term-document matrix or occurrence matrix. LSA is an important extension of vector space model, also known as LSI(Latent Semantic Indexing).

#### Term-Document Matrix

Literally, this matrix represents the occurrence of each word in each document. The rows represent the terms, while the colomns represent the documents. More specifically, after you get those documents, e.g. 100 documents, you could build the corresponding vocabulary, e.g. 5000 words. Each document could be represented as a vector with dimension of 5000, with every element representing the occurrence times of corresponding word. Vice versa, every word could also be represented as a vector with dimension of 100, with element representing the occurrence times of that word in corresponding document.

Now we have a matrix, looks like:


<!--$$\begin{bmatrix}
1&0&3&5&9&\ldots\\
2&1&5&3&4&\ldots\\
3&6&4&2&1\\
\vdots&\vdots&\vdots&\vdots&\vdots&\ldots\end{bmatrix}$$-->

![](https://ws1.sinaimg.cn/large/006tKfTcgy1fi3spjgxwwj31540dqmy3.jpg)

The LSA assumes that words that are closed in meaning should appear with a similar times among documents. In this matrix, if there is a word 'he', which is the row of the matrix, to be (1, 0, 3, 5, 9,...), similary to word 'I'. We're pretty confident that the word 'he' is closed to word 'I'. 

#### Cosine Similarity

To measure the similarity between two words or documents, the common way is to calculate the normalized inner product between two embedding vectors, which could reflect the angle between two vectors.

<!--$$similarity = cos(\theta) = \frac{\mathbf{w_1  \bullet  w_2}}{\|\mathbf{w_1}\|\bullet\|\mathbf{w_2}\|}$$
-->

<div align="center">
<img src="https://latex.codecogs.com/svg.latex?similarity&space;=&space;cos(\theta)&space;=&space;\frac{\mathbf{w_1&space;\bullet&space;w_2}}{\|\mathbf{w_1}\|\bullet\|\mathbf{w_2}\|}" title="similarity = cos(\theta) = \frac{\mathbf{w_1 \bullet w_2}}{\|\mathbf{w_1}\|\bullet\|\mathbf{w_2}\|}" />
</div>



#### Lower matrix rank

Now how do you classify the documents or terms based on this matrix? We are expecting to reduce the dimension of this matrix, that is map it into a lower rank matrix. This approach could solve the data sparse problem and compute a shorten embedding of words and documents. In linear algebra, the singular value decomposition (SVD) could handle this. 

##### - Theorem of SVD


For an <img src="https://latex.codecogs.com/svg.latex?$m\times&space;n$" title="$m\times n$" /> 
real matrix <img src="https://latex.codecogs.com/svg.latex?$\mathbf&space;{M}&space;$" title="$\mathbf {M} $" />,
 whose element comes from field <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{R}" title="\mathbf {R}" />
 . It can be decomposed as form:
<div align = "center"> 
<img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{M=U\Sigma&space;V^{T}}" title="\mathbf {M=U\Sigma V^{T}}" />
</div>

where:

*	<img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{U}" title="\mathbf {U}" />
is an <img src="https://latex.codecogs.com/svg.latex?m\times&space;m" title="m\times m" />
 real	[unitary matrix](https://en.wikipedia.org/wiki/Unitary_matrix), more specifically, orthogonal matrix.
*  <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{V}" title="\mathbf {V}" />
 is an <img src="https://latex.codecogs.com/svg.latex?n\times&space;n" title="n\times n" />
  orthogonal matrix.  
*  <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{\Sigma}" title="\mathbf {V}" />
  is an <img src="https://latex.codecogs.com/svg.latex?m\times&space;n" title="m\times n" /> 
  rectangular diagonal matrix with non-negative real numbers on the diagonal, known as singular values of <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{M}&space;$" title="$\mathbf {M} $" />. 


Note, the decomposition is not unique.
The columns of <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{U}" title="\mathbf {U}" /> and the columns of <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{V}" title="\mathbf {V}" />  are called the left-singular vectors and right-singular vectors of matrix <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{M}" title="\mathbf {M}" />, respectively. The interesting property is:

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\mathbf{M&space;M^{T}}&space;=&space;\mathbf{U&space;\Sigma^{2}&space;U^{T}}" title="\mathbf{M M^{T}} = \mathbf{U \Sigma^{2} U^{T}}" />
</div>

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\mathbf{M^{T}M}&space;=&space;\mathbf{V&space;\Sigma^{2}V^{T}}" title="\mathbf{M^{T}M} = \mathbf{V \Sigma^{2} V^{T}}" />
</div>

where <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{U}" title="\mathbf {U}" />
, <img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{V}" title="\mathbf {V}" />
must contain the eigenvectors of <img src="https://latex.codecogs.com/svg.latex?\mathbf{M&space;M^{T}}" title="\mathbf{M M^{T}}" /> 
and <img src="https://latex.codecogs.com/svg.latex?\mathbf{M^{T}M}" title="\mathbf{M^{T}M}" />
, respectively, since <img src="https://latex.codecogs.com/svg.latex?\Sigma^{2}" title="\Sigma^{2}" />
 is diagonal matrix.

##### - SVD in Term-Document Matrix


The singular value decomposition is used in approximating the matrix <img src="https://latex.codecogs.com/svg.latex?\mathbf{M}" title="\mathbf{M}" /> as SVD of matrices with rank <img src="https://latex.codecogs.com/svg.latex?k" title="k" />, which looks like:

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\begin{bmatrix}&space;x_{1,1}&&space;\ldots&space;&&space;x_{1,n}\\&space;\vdots&space;&\vdots&space;&&space;\vdots&space;\\&space;x_{m,1}&\ldots&x_{m,n}&space;\end{bmatrix}&space;\Rightarrow &space;\begin{bmatrix}\mathbf{u_{1}}&space;&&space;\ldots&space;&&space;\mathbf{u_{k}}\end{bmatrix}&space;\bullet&space;\begin{bmatrix}&space;\sigma_1&\ldots&0\\&space;\vdots&&space;\ddots&\vdots&space;\\&space;0&\ldots&\sigma_k\end{bmatrix}&space;\bullet&space;\begin{bmatrix}\mathbf{v_1}\\\vdots\\\mathbf{v_k}\end{bmatrix}" title="\begin{bmatrix} x_{1,1}& \ldots & x_{1,n}\\ \vdots &\vdots & \vdots \\ x_{m,1}&\ldots&x_{m,n} \end{bmatrix} \Rightarrow  \begin{bmatrix}\mathbf{u_{1}} & \ldots & \mathbf{u_{k}}\end{bmatrix} \bullet \begin{bmatrix} \sigma_1&\ldots&0\\ \vdots& \ddots&\vdots \\ 0&\ldots&\sigma_k\end{bmatrix} \bullet \begin{bmatrix}\mathbf{v_1}\\\vdots\\\mathbf{v_k}\end{bmatrix}" /> 
</div>

where, <img src="https://latex.codecogs.com/svg.latex?\mathbf{u_i}" title="\mathbf{u_i}" /> and <img src="https://latex.codecogs.com/svg.latex?\mathbf{v_i}" title="\mathbf{v_i}" /> are left and right singular vector. 

The original SVD intuitively looks like this:

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\begin{bmatrix}&space;x_{1,1}&&space;\ldots&space;&&space;x_{1,n}\\&space;\vdots&space;&\vdots&space;&&space;\vdots&space;\\&space;x_{m,1}&\ldots&x_{m,n}&space;\end{bmatrix}&space;=&space;\begin{bmatrix}\mathbf{\hat{t}_1^T}}\\\vdots\\\mathbf{\hat{t}_m^T}}\end{bmatrix}&space;\bullet&space;\mathbf{\Sigma}_{m,n}&space;\bullet&space;\begin{bmatrix}\mathbf{\hat{d}_1}&\ldots&\mathbf{\hat{d}_n}\end{bmatrix}"/>
</div>
 


where, the only part of <img src="https://latex.codecogs.com/svg.latex?\mathbf{U}" /> that contributes to <img src="https://latex.codecogs.com/svg.latex?\mathbf{t}^T_i" align = "center"/>is called <img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{t}^T_i}" align = "center" />, similar for <img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{d}_{i}}" align = "center" />. Note, The term vectors <img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{t}^{T}_{i}}" align = "center" />
or doc vectors <img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{d}_{i}}" align = "center" />
are not the eigenvectors, but depends on eigenvectors. Now, our task is to approximate the original matrix <img src="https://latex.codecogs.com/svg.latex?\mathbf{M}"  /> with lower rank matrix <img src="https://latex.codecogs.com/svg.latex?\mathbf{\tilde{M}}"/> , which has a specific rank <img src="https://latex.codecogs.com/svg.latex?k" align = "center"/> . [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) suggests that picking up the largest <img src="https://latex.codecogs.com/svg.latex?k" align = "center"/> singular values, corresponding singular vectors to construct a truncated matrix <img src="https://latex.codecogs.com/svg.latex?\mathbf{\tilde{M}}"  /> has a minimum error.  We can write the approximated decompostion as:
<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\mathbf{X_k}&space;=&space;\mathbf{U_k}\mathbf{\Sigma_k}\mathbf{V_k}^T" title="\mathbf{X_k} = \mathbf{U_k}\mathbf{\Sigma_k}\mathbf{V_k}^T" />
</div>

The row "term" vector <img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{t}}^{T}_{i}" align = "center" /> then has <img src="https://latex.codecogs.com/svg.latex?k" align = "center" /> entries mapping it to a lower-dimensional space dimensions.
 Now, if we'd like compare the similarity between term <img src="https://latex.codecogs.com/svg.latex?i"  align = "center"/>, term <img src="https://latex.codecogs.com/svg.latex?j" align = "center" />
, we can just calculate the cosine similary between <img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{t}_i}^T\math{\Sigma_k}" align = "center" /> and <img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{t}_j}^T\math{\Sigma_k}" align = "center" /> . Or if we'd like compare two documents, <img src="https://latex.codecogs.com/svg.latex?i" align = "center"/> and <img src="https://latex.codecogs.com/svg.latex?j" align = "center"/>, look at the cosine similary these two vectors,  <img src="https://latex.codecogs.com/svg.latex?\math{\Sigma_k}\mathbf{\hat{d}_i}" title="\math{\Sigma_k}\mathbf{\hat{d}_i}" align = "center"/>, <img src="https://latex.codecogs.com/svg.latex?\math{\Sigma_k}\mathbf{\hat{d}_j}" title="\math{\Sigma_k}\mathbf{\hat{d}_j}" align = "center"/>.

Now the problem is how do we calculate the <img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{t}}^{T}_{i}" align = "center"/> and <img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{d}}_{i}" align = "center"/>. Follow the below derivatives:

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\mathbf{t}_i^T&space;=&space;\mathbf{\hat{t}}_i^T\mathbf{\Sigma_k}\mathbf{V_k^T}" title="\mathbf{t}_i^T = \mathbf{\hat{t}}_i^T\mathbf{\Sigma_k}\mathbf{V_k}" />
</div>

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{t}}_i^T&space;=&space;\mathbf{t}_i^T\mathbf{V_k^{-T}}\mathbf{\Sigma_k^{-1}} = &space;\mathbf{t}_i^T\mathbf{V_k}\mathbf{\Sigma_k^{-1}} "  />
</div>

For documents, it will be like:

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\mathbf{\hat{d}}_i = &space;\mathbf{\Sigma_k^{-1}}\mathbf{U_k^T}\mathbf{d}_i "  />
</div>


#### TF-IDF Weights

The term-document matrix that we investigate is the raw count of terms in each documents. We can also use term frequency that is 
<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\frac{n_{t_i,d}}{\sum_i{n_{t_{i},d}}}" assign = "center" /> 
</div>

For now, we are still categorizing the documents by analyzing how many times that words occur. But there is problem, some words, e.g. "the", "a", "this", almost occur every document. Those trival words does not provide useful information in measuring the similarity among documents, while some important, less occurring words, e.g. "movie", "music", "sport" do play an important role. So it is suggested to use TF-IDF weights to construct the Term-Document matrix. 

The TF here is just the term-frequency as above, while the inverse document frequency measure the importance of specific term, as belows:

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?idf(t,D)&space;=&space;log\frac{N}{|&space;\{d\in&space;D:&space;t&space;\in&space;d&space;\}|&space;}" title="idf(t,D) = log\frac{N}{| \{d\in D: t \in d \}| }" />
</div>
with

* <img src="https://latex.codecogs.com/svg.latex?N" assign = "center"/>: total number of documents in the corpus <img src="https://latex.codecogs.com/svg.latex?N&space;=&space;|D|" align = "center"/>.
* <img src="https://latex.codecogs.com/svg.latex?|\{d\in&space;D:&space;t\in&space;d\}|" align = "center" />: number of documents where the term  <img src="https://latex.codecogs.com/svg.latex?t"/> appears. If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to <img src="https://latex.codecogs.com/svg.latex?1&space;&plus;&space;|\{d&space;\in&space;D:&space;t&space;\in&space;d\}|" title="1 + |\{d \in D: t \in d\}|" align = "center"/>.

It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. That means the more the "specific word" occurs in document <img src="https://latex.codecogs.com/svg.latex?i"/> and the less documents contain this "specific word" , the more important that "specific word" is to this document <img src="https://latex.codecogs.com/svg.latex?i" />.



#### R codes

Finally, we have implementation, which is computed with R. 

```
# load packages
library(dbscan)
library(magrittr)
library(dplyr)
library(lsa)  
library(tidytext) 

# load text file to be analyzed
data <- fread("~/data.txt", sep = '\n', 
                 header = F, col.names = "documents")

# load stop words document
stop_words <- fread("~/stop_words.txt", sep = '\n', header = F,
                    col.names = "words")
                    
# documents index
data = data[, doc := c(1:dim(data)[1])]
  
# clean data
data[, documents := tolower(documents)] 
data[, documents := str_replace_all(documents, "[,.'?!:-]", " ")]   

# unnest words in documents
words_doc <- unnest_tokens(data, word, documents) 
  
words_doc <- words_doc[word != "" & !(word %in% stop_words),]

words_doc <- words_doc[, .N, by = c("doc", "word")]     

# create the doc-term matri x(transpose of term-doc matrix)
doc_term_matrix <- cast_dtm(words_doc, doc, word, N, weighting = tm::weightTf)
  
# perform lsa
mylsa <- lsa(doc_term_matrix)

# cluster the documents using DBSCAN
db_clust <- dbscan(mylsa$tk, 		# you can also pick part of factors
                   eps = db_eps, 	# define appropriate eps 
                   MinPts = db_minpoints)
                   
# add the cluster of documents into original data
data[, db_cluster := as.factor(db_clust$cluster)]

```

#### Other Application

The LSA could also be utilized for information retrieval, which is first implemented in 1988. Since we can get the embedding of documents, we can match documents by calculating the cosine similarity between documents. Howevere, the queries that we'd like to check are mostly new documents. We can try to use
<img src="https://latex.codecogs.com/svg.latex?\mathbf&space;{M=U\Sigma&space;V^{T}}" assign = "center" /> equation to add new vectors. For a new document 
<img src="https://latex.codecogs.com/svg.latex?d" />, we can create a new column in <img src="https://latex.codecogs.com/svg.latex?\mathbf{A}" /> using the original weight functions and parameters, then multiply it by <img src="https://latex.codecogs.com/svg.latex?\mathbf{U\Sigma^{-1}}" assign = "center"/>.

#### Drawbacks of LSA
* LSA can't reflect the order of words, since we have decomposed the documents into word frequecy of words vocabulary.
* LSA can't capture polysemy of words. It treat the word with different meaning as same one. The special meaning of word can't be reflected.
* Inappropriate interprete the meaning. Some words have similar meaning in specific situation, while completely different in other situations. For example, "iced" + "cream"  = "ice-cream", while "iced" + "moisturizer" = "?".




#### Reference
* LSA:	[https://en.wikipedia.org/wiki/Latent_semantic_analysis]()