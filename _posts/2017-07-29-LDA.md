---
layout: post
title: How does LDA assign probability of different topics to documents?
subtitle: Using Bayesian to predict the probabilty of each topic for documents.
bigimg: /img/ducks.jpg
---

> Last post, we went through the LSA to get a embed matrix. However, clustering documents shows an obvious shortage, since documents may have multiple themes. This post introduces the LDA which utilizes the Bayesian inference to get the posterior probability of topics in each document, also the posterior probability of words in each topic.


Latent Dirichlet allocation (LDA) is an example of a topic model and was first presented as a graphical model for topic discovery. The LDA allows multiple topics for each document, by showing the probablilty of each topic. For example, a document may have 90% probability of topic A and 10% probability of topic B. Also it also analyze the probability of words in each topic. For example, the word "summer" is with 15% probability in topic A.

There are two posterior probability that LDA cares about: 

*  The probability of topics in each document.
*  The probability of words in each topic.

Since it utilizes the Bayesian inference, symbol notations are more appropriate to elucidate the conditional probability. 

#### Bayesian equation

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\mathbf{P(A|B)&space;=&space;\frac{P(A\cap B)}{P(B)}=\frac{P(B|A)\times&space;P(A)}{P(B)}}" title="P(A|B) = \frac{P(A,B)}{P(B)}=\frac{P(B|A)\times P(A)}{P(B)}" />
</div>

#### LDA Model

LDA assumes the following generative process for a corpus <img src="https://latex.codecogs.com/svg.latex?D"/> consisting of <img src="https://latex.codecogs.com/svg.latex?M"  />  documents each of length <img src="https://latex.codecogs.com/svg.latex?N_{i}"  align = "center"  /> and <img src="https://latex.codecogs.com/svg.latex?K" /> available topics. There are 4 priors assumed,

1. We assume topic distribution per document <img src="https://latex.codecogs.com/svg.latex?\mathbf{\theta_{d}}" align = "center" /> follows <img src="https://latex.codecogs.com/svg.latex?\mathbf{Dir_K(\alpha)}" align = "center" />.
2.  We assume words distribution per topic <img src="https://latex.codecogs.com/svg.latex?\mathbf{\varphi_k}" align = "center" /> follows <img src="https://latex.codecogs.com/svg.latex?\mathbf{Dir_V(\beta)}" align = "center" />.
3. The topic  <img src="https://latex.codecogs.com/svg.latex?\mathbf{z_{dw}}" align = "center" />
for document <img src="https://latex.codecogs.com/svg.latex?d"  />, postion <img src="https://latex.codecogs.com/svg.latex?w" align = "center" /> follows the <img src="https://latex.codecogs.com/svg.latex?\mathbf{Categorical_K(\theta_d)}" align = "center" />.
4. The word <img src="https://latex.codecogs.com/svg.latex?\mathbf{w_{dw}}" align = "center" /> for document <img src="https://latex.codecogs.com/svg.latex?d"  />, position <img src="https://latex.codecogs.com/svg.latex?w" align = "center" /> follow the <img src="https://latex.codecogs.com/svg.latex?\mathbf{Categorical_V(\varphi_{z_{dw}})}" align = "center" />

where, <img src="https://latex.codecogs.com/svg.latex?d&space;\in&space;\{1,2,\ldots,M\}" align = "center" />, <img src="https://latex.codecogs.com/svg.latex?w&space;\in&space;\{1,2,\ldots,N_i\}" align = "center" /> , <img src="https://latex.codecogs.com/svg.latex?k&space;\in&space;\{1,2,\ldots,K\}" align = "center" />, and
 
<img src="https://latex.codecogs.com/svg.latex?V"/> denotes the vocabulary size of total words.

The more detailed derivations could be found in Wiki, [Inference](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Inference).

