---
layout: post
title: How does Word2Vec embed the word?
subtitle: Dig out words relationships using Word2Vec
bigimg: /img/ducks.jpg
---


> In last post, we investigate the LSA to classify the documents. The output results are the embedding of terms and documents, with which we can classify terms and documents using cluster techniques. The LSA or LDA all belong to count-based methods of Vector Space Models. VSMs have a long, rich history in NLP, but all methods depend in some way or another on the Distributional Hypothesis, which states that words that appear in the same contexts share semantic meaning. This post introduces predictive methods of VSMs -- Word2Vec. Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors. ([vector represent of words](https://www.tensorflow.org/tutorials/word2vec))

Word2Vec is a shallow, two layers Neural Network that could analyze the raw text and produce a vector space of words. There are two architectures:  continuous bag-of-words (CBOW) or continuous skip-gram.
In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words, while in the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). This distinction can be visualized as below:

![](https://ws4.sinaimg.cn/large/006tNc79gy1fi4sqss6gqj314i0d00u6.jpg)

![](https://ws1.sinaimg.cn/large/006tNc79gy1fi4zoz03j3j31aq0hs40e.jpg)

Note, the skip-gram shown above is assumed to be with 0-skip-2-grams model, with only directly two neighbors considered. We could also produce 1-skip-2-grams, which would in addition include ("lunch", "salad"), ("lunch", "would"), ("as", "I"), ("as","have")...etc. besides the 0-skip pairs before.

## Embedding

### One-hot Vector

For all sentences, We'd like to represent each word of specific sentence as a vector. The simplest way is to use the one-hot vector like (1, 0, 0, ...). More specifically, for thousands of sentences, We tokenize the sentence into words. Put all words into a set, we get words vocabulary of sentences. Visualized as follows:

![](https://ws3.sinaimg.cn/large/006tNc79gy1fi4spntxt0j30u40cejrz.jpg)



Note the dictionary of vocabulary has order, once set, the index of word does not change. If there is a sentence, like 'I love salad'. The 'I' is at <img src="https://latex.codecogs.com/svg.latex?2_{nd}" align = "center" />
 place of vocabulary, so the initial embedding vector of 'I' is <img src="https://latex.codecogs.com/svg.latex?(0,&space;1,&space;0,&space;0,&space;\ldots)^T" align = "center" />. Similarly, the embedding vector of 'love' is <img src="https://latex.codecogs.com/svg.latex?(0,&space;0,&space;0,&space;1,&space;0,&space;\ldots)^T" align = "center" /> and initial embedding of "salad" is <img src="https://latex.codecogs.com/svg.latex?(0,&space;0,&space;0,&space;0,&space;0,&space;1&space;,\ldots)^T" align = "center" />. The length of the vector is the vocabulary size of queries. So the embedding of a sentence should be a <img src="https://latex.codecogs.com/svg.latex?vocab\_size&space;\times&space;3" align = "center" /> matrix like,

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\begin{bmatrix}&space;0&0&0&space;\\&space;1&0&0&space;\\&space;0&0&0&space;\\0&1&0&space;\\0&0&0&space;\\&space;0&0&1\\\vdots&\vdots&\vdots&space;\end{bmatrix}"  align = "center" /> </div>
 


The 3 in dimension <img src="https://latex.codecogs.com/svg.latex?vocab\_size&space;\times&space;3" align = "center" /> is the length of sentence.

In fact, when we construct the model with coding. We don't need to translate all words into this one-hot vector. That costs time and memory. In fact, when a matrix with dimension <img src="https://latex.codecogs.com/svg.latex?4&space;\times&space;vocab\_size" align = "center" />, like 
<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?\begin{bmatrix}&space;0.01&0.02&0.03&0.04&0.05&\ldots&space;\\0.04&0.05&0.06&0.07&0.08&\ldots&space;\\0.07&0.08&0.09&0.10&0.11&\ldots&space;\\0.12&0.13&0.14&0.15&0.16&\dots&space;\end{bmatrix}" title="\begin{bmatrix} 0.01&0.02&0.03&0.04&0.05&\ldots \\0.04&0.05&0.06&0.07&0.08&\ldots \\0.07&0.08&0.09&0.10&0.11&\ldots \\0.12&0.13&0.14&0.15&0.16&\dots \end{bmatrix}" />
</div>
right multiplied by the embedding of a word, like <img src="https://latex.codecogs.com/svg.latex?\begin{bmatrix}0&1&0&0&\ldots\end{bmatrix}^T" align = "center" />, we just get the <img src="https://latex.codecogs.com/svg.latex?2_{nd}" align = "center" /> column of the matrix <img src="https://latex.codecogs.com/svg.latex?\begin{bmatrix}0.02&0.05&0.08&0.13\end{bmatrix}^T" align = "center" />
. Thus, normally we just embed the sentence 'I love salad' as <img src="https://latex.codecogs.com/svg.latex?\begin{bmatrix}2&4&6&space;\end{bmatrix}^T" align = "center" />, which are the index of words in vocabulary. The 4 in dimension <img src="https://latex.codecogs.com/svg.latex?4&space;\times&space;vocab\_size" align = "center" /> is the hidden_size (how many nerons we use in neural network) that we will talk about later.


## Word2Vec

[Word2Vec](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)

Now, imagine a model that we want to predict surrounding word given a center word. Like in sentence 'I love salad', for the word 'I', there could be pair ('I', 'love'), ('I', 'salad'). For all of thousands sentences, we may get ten thousands of pairs or more. In fact, for 'I love salad', we could produce 6 pairs, if we choose 0-skip-2-grams model.  This can be treated as classification problem, with a word as input and vocabulary size target words as classes. The values in the output vector could represent the probability that the words are nearby the input word. In this model, We set one hidden layer, with 4 neurons in hidden layer (hidden size is 4). Larger hidden size is better, but it's hard to edit for me. \* . \*


There is a parameter matrix and activation function that map the inputs to the hidden states. Assume the matrix is $\theta$ as the one I give last module and the activation function g(.) is softmax function, then the first neuron state is like:

<div align = "center">

<img src="https://latex.codecogs.com/svg.latex?0.23761045&space;=&space;g(0.02)&space;=&space;g(\begin{bmatrix}&space;0.01&0.02&0.03&0.04&0.05&\ldots&space;\end{bmatrix}&space;\times&space;\begin{bmatrix}0\\1\\0\\0\\\vdots\end{bmatrix})"  />

</div>

<div align = "center">
<img src="https://latex.codecogs.com/svg.latex?0.24484676&space;=&space;g(0.05)&space;=&space;g(\begin{bmatrix}&space;0.04&0.05&0.06&0.07&0.08&\ldots&space;\end{bmatrix}&space;\times&space;\begin{bmatrix}0\\1\\0\\0\\\vdots\end{bmatrix})" title="0.24484676 = g(0.05) = g(\begin{bmatrix} 0.04&0.05&0.06&0.07&0.08&\ldots \end{bmatrix} \times \begin{bmatrix}0\\1\\0\\0\\\vdots\end{bmatrix})" />
</div>
...

Those calculations are the same from hidden layer to output layer. All the pairs are used to train the model to minimize the loss between outputs and actural targets. Ultimately, we could get updated parameter matrices. Then using these matrices, we can predict the words around a given center word.

This is just Word2Vec, which can be used to embed words and represent the relationship among words. The Word2Vec is a shallow neural network.